# Small TRM configuration for MPS with float32 precision
# This config uses float32 instead of bfloat16 for numerical stability on Apple Silicon

name: recursive_reasoning.trm:TinyRecursiveReasoningModel_ACTV1

# Smaller model dimensions
hidden_size: 512
expansion: 2.0          # Reduced from 4.0
num_heads: 8            # Reduced from 8

# Fewer reasoning cycles
H_cycles: 2             # Reduced from 3
L_cycles: 2             # Reduced from 4
H_layers: 0             # Not used
L_layers: 2             # Reduced from 4

# Positional encodings
pos_encodings: rope     # RoPE is memory efficient
rope_theta: 10000.0

# Puzzle embeddings (not used for chat)
puzzle_emb_ndim: 0
puzzle_emb_len: 0

# Normalization
rms_norm_eps: 1.0e-5

# ACT (Adaptive Computation Time) config
halt_max_steps: 4       # Allow up to 4 computation steps (H_cycles * L_cycles = 2 * 2 = 4)
halt_exploration_prob: 0.1  # 10% exploration during training

# Use causal attention for chat models
causal: true

# MLP on L level (more memory efficient than attention)
mlp_t: true

# No continue ACT loss
no_ACT_continue: true

# Precision - CRITICAL: Use float32 for MPS stability
forward_dtype: float32  # MPS has buggy bfloat16 support, use float32 instead

# Loss configuration
loss:
  name: losses:ACTLossHead
  loss_type: softmax_cross_entropy  # Use standard softmax for numerical stability
