# Fast Chat Model Configuration with float32 precision
# Uses float32 instead of bfloat16 for numerical stability on Apple Silicon MPS

defaults:
  - arch: trm_small_fp32  # Use float32 architecture for MPS stability
  - _self_

hydra:
  output_subdir: null

# Data path - use smaller dataset
data_paths: ['data/alpaca-512']  # Smaller dataset, shorter sequences
data_paths_test: []
dataset_type: "chat"

evaluators: []

# Training hyperparameters
global_batch_size: 8  # Larger batch size
gradient_accumulation_steps: 4  # Less accumulation needed

epochs: 100  # Just 5 epochs for fast iteration
eval_interval: 1  # Checkpoint every epoch
checkpoint_every_eval: True

# Learning rate schedule
lr: 0.0001
grad_clip_norm: 1.0
lr_min_ratio: 0.1
lr_warmup_steps: 100  # Shorter warmup

# Optimizer settings
beta1: 0.9
beta2: 0.95
weight_decay: 0.1

# Puzzle embedding settings (not used for chat)
puzzle_emb_weight_decay: 0.1
puzzle_emb_lr: 1e-2

# Training settings
seed: 0
min_eval_interval: 0

# EMA disabled for speed
ema: False

# Weight freezing
freeze_weights: False
